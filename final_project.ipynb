{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Data_loader(Dataset):\n",
    "    def __init__(self, data_frame, img_dir, transform=None, is_test=False):\n",
    "        \"\"\"\n",
    "        自定义的 PyTorch Dataset，用于加载图像和相关的标签数据。\n",
    "\n",
    "        参数:\n",
    "        - data_frame (pandas.DataFrame): 包含图像文件名和标签的 DataFrame。\n",
    "        - img_dir (str): 图像文件夹路径。\n",
    "        - transform (callable, optional): 图像变换（数据增强等）。\n",
    "        - is_test (bool): 是否为测试集标志。如果是测试集，没有标签。\n",
    "        \"\"\"\n",
    "        self.data_frame = data_frame  # 直接使用传入的DataFrame\n",
    "        self.img_dir = img_dir  # 图像文件夹路径\n",
    "        self.transform = transform  # 图像变换\n",
    "        self.is_test = is_test  # 是否是测试集\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集的大小\"\"\"\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"根据索引获取图像和标签\"\"\"\n",
    "        img_name = os.path.join(self.img_dir, str(self.data_frame.iloc[idx, 0]) + \".jpg\")  # 假设第一列是图像文件名\n",
    "        image = Image.open(img_name).convert(\"RGB\")  # 打开图像并转换为RGB格式\n",
    "\n",
    "        if self.is_test:\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image  # 测试集没有标签，返回图像即可\n",
    "        else:\n",
    "            label = self.data_frame['stable_height'].iloc[idx]  # 使用 stable_height 作为标签\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义图像预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 加载并划分训练数据\n",
    "train_csv_path = 'data/train.csv'\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "\n",
    "# 使用 train_test_split 划分训练集和验证集\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.99, random_state=42)\n",
    "\n",
    "# 创建训练集和验证集的数据加载器\n",
    "train_img_dir = 'data/train/'\n",
    "train_dataset = Data_loader(train_df, train_img_dir, transform=transform)\n",
    "val_dataset = Data_loader(val_df, train_img_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 加载测试数据\n",
    "test_csv_path = 'data/test.csv'\n",
    "test_img_dir = 'data/test/'\n",
    "test_data = pd.read_csv(test_csv_path)\n",
    "test_dataset = Data_loader(test_data, test_img_dir, transform=transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新的验证集数量: 10\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "# 假设你只想取前1000个验证集样本\n",
    "subset_indices = list(range(10))  # 选择前1000个样本的索引\n",
    "val_subset = Subset(val_dataset, subset_indices)\n",
    "\n",
    "# 创建新的 DataLoader\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 打印新的验证集数量\n",
    "print(f\"新的验证集数量: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量: 76\n",
      "验证集数量: 7604\n",
      "测试集数量: 1920\n"
     ]
    }
   ],
   "source": [
    "print(f\"训练集数量: {len(train_dataset)}\")\n",
    "print(f\"验证集数量: {len(val_dataset)}\")\n",
    "print(f\"测试集数量: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 从训练数据加载器中取出一个 batch\n",
    "# data_iter = iter(train_loader)\n",
    "# images, labels = next(data_iter)\n",
    "\n",
    "# # 打印批次中的图像和标签\n",
    "# print(f\"图像批次大小: {images.size()}\")  # 图像的张量大小，例如 [32, 3, 128, 128] 表示32张图像，每张图像是3通道，128x128\n",
    "# print(f\"标签: {labels}\")  # 打印标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # 可视化一个批次的图像\n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5  # 将图像还原\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "# # 获取一个批次的数据\n",
    "# data_iter = iter(train_loader)\n",
    "# images, labels = next(data_iter)\n",
    "\n",
    "# # 显示图像\n",
    "# imshow(images[0])  # 显示批次中的第一张图像\n",
    "# print(f\"标签: {labels[0]}\")  # 打印第一张图像的标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 模型\n",
    "层数 3\n",
    "kernel numbers: 32 64 128\n",
    "kernel size: 3 x 3\n",
    "active function: relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        # 定义3层卷积层和最大池化层\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # 修改后的全连接层\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "        # Dropout层用于避免过拟合\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        \n",
    "        # 展平输入\n",
    "        x = x.view(x.size(0), -1)  # 动态展平，确保批次大小正确\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # print(f\"模型输出的形状: {x.shape}\")  # 打印输出的形状\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用预训练的 ResNet50 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet50_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50_Model, self).__init__()\n",
    "        # 加载预训练的 ResNet50 模型\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        \n",
    "        # 替换 ResNet50 的全连接层\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # 最终输出一个标量，用于回归任务\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f\"模型输出的形状: {x.shape}\")  # 打印输出的形状\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNeXt101(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNeXt101, self).__init__()\n",
    "        # 加载预训练的 ResNeXt101 模型\n",
    "        self.resnet = models.resnext101_64x4d(weights=models.ResNeXt101_64X4D_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        # 替换全连接层\n",
    "        self.resnet.fc = nn.Identity()\n",
    "\n",
    "        # 定义新的全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)  # 输出单个标量\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.fc(x)\n",
    "        # print(f\"模型输出的形状: {x.shape}\")  # 打印输出的形状\n",
    "        return x\n",
    "    \n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        # 加载预训练的 ResNet18 模型\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        # 替换 ResNet18 的全连接层\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # 定义新的全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # 输出单个标量\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.fc(x)\n",
    "        # print(f\"模型输出的形状: {x.shape}\")  # 打印输出的形状\n",
    "        return x\n",
    "        \n",
    "# 据说这个最好    \n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        # 加载预训练的 GoogLeNet 模型\n",
    "        self.googlenet = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.googlenet.fc.in_features\n",
    "        # 替换 GoogLeNet 的全连接层\n",
    "        self.googlenet.fc = nn.Identity()\n",
    "\n",
    "        # 定义新的全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)  # 输出单个标量\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.googlenet(x)\n",
    "        x = self.fc(x)\n",
    "        # print(f\"模型输出的形状: {x.shape}\")  # 打印输出的形状\n",
    "        return x\n",
    "    \n",
    "class ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViT, self).__init__()\n",
    "        # 加载预训练的 ViT 模型\n",
    "        self.vit = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.vit.heads.head.in_features\n",
    "        # 替换 ViT 的全连接层\n",
    "        self.vit.heads = nn.Identity()\n",
    "\n",
    "        # 定义新的全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 125),\n",
    "            nn.BatchNorm1d(125),  # 添加正则化层\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(125, 1)  # 将输入调整为 125 以匹配上面的输出\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)\n",
    "        x = self.fc(x)\n",
    "        # print(f\"模型输出的形状: {x.shape}\")  # 打印输出的形状\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader,model_name, criterion=None, optimizer=None, num_epochs=10, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        初始化训练类，准备模型、数据和相关参数\n",
    "\n",
    "        参数:\n",
    "        - model: 待训练的模型\n",
    "        - train_loader: 训练数据的 DataLoader\n",
    "        - val_loader: 验证数据的 DataLoader\n",
    "        - criterion: 损失函数\n",
    "        - optimizer: 优化器\n",
    "        - num_epochs: 训练的 epoch 数\n",
    "        - learning_rate: 学习率\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.model_name = model_name\n",
    "        self.criterion = criterion if criterion else nn.MSELoss() # MSEloss 作为损失函数\n",
    "        # self.criterion = nn.CrossEntropyLoss() # CrossEntropyLoss 作为损失函数\n",
    "        self.optimizer = optimizer if optimizer else optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        训练一个 epoch 的数据，并返回训练损失\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}', unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device).float()\n",
    "\n",
    "            # 前向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            # 反向传播与优化\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # print(\"one epoch----\")\n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        # print(\"one epoch end----\")\n",
    "        return avg_loss\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        在验证集上进行评估，返回验证损失\n",
    "        \"\"\"\n",
    "        # print(\"validate start----\")\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device).float()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.squeeze(), labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(self.val_loader)\n",
    "        # print(\"validate end----\")\n",
    "        return avg_val_loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        训练模型，并在每个 epoch 结束后进行验证\n",
    "        \"\"\"\n",
    "        log_dir = self.create_log_dir()  # 创建日志文件夹\n",
    "        writer = SummaryWriter(log_dir)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # 训练一个 epoch\n",
    "            train_loss = self.train_one_epoch(epoch)\n",
    "            val_loss = self.validate()\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "            \n",
    "            # 记录损失值\n",
    "            writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "            writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "            # print(\"train epoch----\")\n",
    "            # 保存最好的模型\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # print(\"save------\")\n",
    "                self.save_model(f'models/best_model_{self.model_name}.pth')\n",
    "        # print(\"train end -----\")\n",
    "        writer.close()\n",
    "\n",
    "    def create_log_dir(self):\n",
    "        \"\"\"\n",
    "        创建日志文件夹\n",
    "        \"\"\"\n",
    "        from datetime import datetime\n",
    "        current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        log_dir = f'runs/experiment_{current_time}'\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        return log_dir\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        \"\"\"\n",
    "        保存模型到指定路径\n",
    "        \"\"\"\n",
    "        if not os.path.exists('models'):\n",
    "            os.makedirs('models')\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        print(f\"模型已保存到 {model_path}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN_Model()  # 例如 CNN 模型\n",
    "# trainer = ModelTrainer(model, train_loader, val_loader,'CNN_Model', num_epochs=1, learning_rate=0.001)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  33%|███▎      | 1/3 [00:27<00:55, 27.83s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  67%|██████▋   | 2/3 [00:54<00:27, 27.30s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3/3 [00:57<00:00, 19.08s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n",
      "one epoch end----\n",
      "validate start----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate end----\n",
      "Epoch [1/1], Train Loss: 6.9639, Validation Loss: 2.5151\n",
      "train epoch----\n",
      "save------\n",
      "模型已保存到 models/best_model_ResNeXt101.pth\n",
      "train end -----\n"
     ]
    }
   ],
   "source": [
    "# model = ResNeXt101() \n",
    "# trainer = ModelTrainer(model, train_loader, val_loader,'ResNeXt101', num_epochs=1, learning_rate=0.001)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  33%|███▎      | 1/3 [00:01<00:03,  1.85s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  67%|██████▋   | 2/3 [00:03<00:01,  1.83s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3/3 [00:03<00:00,  1.33s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n",
      "one epoch end----\n",
      "validate start----\n",
      "validate end----\n",
      "Epoch [1/1], Train Loss: 6.6761, Validation Loss: 5.0824\n",
      "train epoch----\n",
      "save------\n",
      "模型已保存到 models/best_model_ResNet18.pth\n",
      "train end -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = ResNet18() \n",
    "# trainer = ModelTrainer(model, train_loader, val_loader,'ResNet18', num_epochs=1, learning_rate=0.001)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  33%|███▎      | 1/3 [00:01<00:03,  1.99s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  67%|██████▋   | 2/3 [00:03<00:01,  1.96s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3/3 [00:04<00:00,  1.42s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n",
      "one epoch end----\n",
      "validate start----\n",
      "validate end----\n",
      "Epoch [1/1], Train Loss: 5.9321, Validation Loss: 11.5069\n",
      "train epoch----\n",
      "save------\n",
      "模型已保存到 models/best_model_GoogLeNet.pth\n",
      "train end -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = GoogLeNet() \n",
    "# trainer = ModelTrainer(model, train_loader, val_loader,'GoogLeNet', num_epochs=1, learning_rate=0.001)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  33%|███▎      | 1/3 [00:05<00:10,  5.28s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  67%|██████▋   | 2/3 [00:09<00:04,  4.53s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3/3 [00:10<00:00,  3.54s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one epoch----\n",
      "one epoch end----\n",
      "validate start----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate end----\n",
      "Epoch [1/1], Train Loss: 11.2113, Validation Loss: 8.6611\n",
      "train epoch----\n",
      "save------\n",
      "模型已保存到 models/best_model_ViT.pth\n",
      "train end -----\n"
     ]
    }
   ],
   "source": [
    "# model = ViT() \n",
    "# trainer = ModelTrainer(model, train_loader, val_loader,'ViT', num_epochs=1, learning_rate=0.001)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(models, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    训练传入的所有模型，并保存每个模型的最佳权重。\n",
    "\n",
    "    参数:\n",
    "    - models: 一个包含所有模型结构的字典，key 为模型名称，value 为模型实例。\n",
    "    - train_loader: 训练数据的 DataLoader。\n",
    "    - val_loader: 验证数据的 DataLoader。\n",
    "    - num_epochs: 每个模型训练的 epoch 数。\n",
    "    - learning_rate: 学习率。\n",
    "    \"\"\"\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"正在训练模型: {model_name}\")\n",
    "        trainer = ModelTrainer(model, train_loader, val_loader, model_name, num_epochs=num_epochs, learning_rate=learning_rate)\n",
    "        trainer.train()\n",
    "        print(f\"模型 {model_name} 训练完成并保存。\")\n",
    "\n",
    "# 使用字典来存储你要训练的模型\n",
    "models = {\n",
    "    'CNN_Model': CNN_Model(),\n",
    "    'ResNet50_Model': ResNet50_Model(),\n",
    "    'ResNeXt101': ResNeXt101(),\n",
    "    'ResNet18': ResNet18(),\n",
    "    'GoogLeNet': GoogLeNet(),\n",
    "    'ViT': ViT(),\n",
    "}\n",
    "\n",
    "# 调用函数来训练所有的模型\n",
    "train_all_models(models, train_loader, val_loader, num_epochs=1, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model, model_path, device=None):\n",
    "        \"\"\"\n",
    "        初始化预测类，加载模型\n",
    "\n",
    "        参数:\n",
    "        - model: 要加载的模型结构（未加载权重）\n",
    "        - model_path: 保存的模型权重路径\n",
    "        - device: 使用的设备（默认为 None，将自动选择 CPU 或 GPU）\n",
    "        \"\"\"\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path))  # 加载模型权重\n",
    "        self.model.eval()  # 设置模型为评估模式\n",
    "        print(f\"模型已从 {model_path} 加载\")\n",
    "\n",
    "    def predict(self, test_loader, output_file='predictions/submission.csv'):\n",
    "        \"\"\"\n",
    "        使用训练好的模型对测试集进行预测，并生成提交文件。\n",
    "\n",
    "        参数:\n",
    "        - test_loader: 测试数据加载器\n",
    "        - output_file: 生成的提交文件名\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        ids = []\n",
    "        with torch.no_grad():\n",
    "            for inputs in test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                predicted_heights = outputs.squeeze().cpu().numpy()  # 将预测结果转为 numpy 数组\n",
    "                predictions.extend(predicted_heights)\n",
    "\n",
    "        # 读取测试集的id并生成提交文件\n",
    "        test_csv = pd.read_csv('data/test.csv')\n",
    "        submission = pd.DataFrame({\n",
    "            'id': test_csv['id'],\n",
    "            'stable_height': predictions\n",
    "        })\n",
    "        submission.to_csv(output_file, index=False)\n",
    "        print(f\"提交文件已生成: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/cm_br_015_9864d7n_0rkhsm0000gn/T/ipykernel_68895/3926736127.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path))  # 加载模型权重\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已从 models/best_model_ViT.pth 加载\n",
      "提交文件已生成: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 假设你已经有一个训练好的模型 CNN_Model() 和 test_loader\n",
    "model = ViT()  # 你的模型结构\n",
    "\n",
    "# 初始化 Predictor 类，传入模型和模型权重路径\n",
    "predictor = Predictor(model=model, model_path='models/best_model_ViT.pth')\n",
    "\n",
    "# 使用 predictor 对测试集进行预测并生成提交文件\n",
    "predictor.predict(test_loader=test_loader, output_file='predictions/submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义模型和对应的模型路径\n",
    "models_info = {\n",
    "    'CNN_Model': ('models/best_model_CNN_Model.pth', CNN_Model()),\n",
    "    'ResNet50_Model': ('models/best_model_ResNet50_Model.pth', ViT()),\n",
    "    'ViT': ('models/best_model_ViT.pth', ViT()),  # ViT 模型\n",
    "    'ResNet18': ('models/best_model_ResNet18.pth', ResNet18()),  # ResNet18 模型\n",
    "    'ResNeXt101': ('models/best_model_ResNeXt101.pth', ResNeXt101()),  # ResNeXt101 模型\n",
    "    'GoogLeNet': ('models/best_model_GoogLeNet.pth', GoogLeNet()),  # GoogLeNet 模型\n",
    "    # 你可以继续添加其他模型\n",
    "}\n",
    "\n",
    "# 创建一个函数，使用 Predictor 进行预测\n",
    "def run_prediction_for_models(models_info, test_loader, output_folder='predictions'):\n",
    "    # 确保输出文件夹存在\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for model_name, (model_path, model_instance) in models_info.items():\n",
    "        print(f\"正在预测模型: {model_name}\")\n",
    "        \n",
    "        # 初始化 Predictor 类，传入模型和模型权重路径\n",
    "        predictor = Predictor(model=model_instance, model_path=model_path)\n",
    "        \n",
    "        # 使用 predictor 对测试集进行预测并生成提交文件\n",
    "        output_file = f\"{output_folder}/submission_{model_name}.csv\"\n",
    "        predictor.predict(test_loader=test_loader, output_file=output_file)\n",
    "        \n",
    "        print(f\"模型 {model_name} 的预测完成并保存至: {output_file}\")\n",
    "\n",
    "# 假设你已经有 test_loader\n",
    "run_prediction_for_models(models_info, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义图像预处理和数据增强\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    transforms.RandomRotation(20),  # 随机旋转20度以内\n",
    "    transforms.RandomResizedCrop(224),  # 随机裁剪并调整为224x224\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 调整亮度、对比度、饱和度、色调\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化\n",
    "])\n",
    "\n",
    "# 验证集和测试集通常不需要数据增强，只需要对数据进行预处理\n",
    "transform_val_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 直接调整图像大小\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化\n",
    "])\n",
    "\n",
    "# 加载并划分训练数据\n",
    "train_csv_path = 'data/train.csv'\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "\n",
    "# 使用 train_test_split 划分训练集和验证集\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建训练集和验证集的数据加载器\n",
    "train_img_dir = 'data/train/'\n",
    "train_dataset = Data_loader(train_df, train_img_dir, transform=transform_train)  # 使用数据增强的transform\n",
    "val_dataset = Data_loader(val_df, train_img_dir, transform=transform_val_test)  # 验证集使用不带数据增强的transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 加载测试数据\n",
    "test_csv_path = 'data/test.csv'\n",
    "test_img_dir = 'data/test/'\n",
    "test_data = pd.read_csv(test_csv_path)\n",
    "test_dataset = Data_loader(test_data, test_img_dir, transform=transform_val_test, is_test=True)  # 测试集使用同样的transform\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 192/192 [02:35<00:00,  1.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 302.9137, Validation Loss: 313.5047\n",
      "模型已保存到 models/best_model_CNN_Model.pth\n"
     ]
    }
   ],
   "source": [
    "# model = CNN_Model()  # 例如 CNN 模型\n",
    "# trainer = ModelTrainer(model, train_loader, val_loader,'CNN_Model', num_epochs=1, learning_rate=0.001)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(models, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    训练传入的所有模型，并保存每个模型的最佳权重。\n",
    "\n",
    "    参数:\n",
    "    - models: 一个包含所有模型结构的字典，key 为模型名称，value 为模型实例。\n",
    "    - train_loader: 训练数据的 DataLoader。\n",
    "    - val_loader: 验证数据的 DataLoader。\n",
    "    - num_epochs: 每个模型训练的 epoch 数。\n",
    "    - learning_rate: 学习率。\n",
    "    \"\"\"\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"正在训练模型: {model_name}\")\n",
    "        trainer = ModelTrainer(model, train_loader, val_loader, model_name, num_epochs=num_epochs, learning_rate=learning_rate)\n",
    "        trainer.train()\n",
    "        print(f\"模型 {model_name} 训练完成并保存。\")\n",
    "\n",
    "# 使用字典来存储你要训练的模型\n",
    "models = {\n",
    "    'CNN_Model': CNN_Model(),\n",
    "    'ResNet50_Model': ResNet50_Model(),\n",
    "    'ResNeXt101': ResNeXt101(),\n",
    "    'ResNet18': ResNet18(),\n",
    "    'GoogLeNet': GoogLeNet(),\n",
    "    'ViT': ViT(),\n",
    "}\n",
    "\n",
    "# 调用函数来训练所有的模型\n",
    "train_all_models(models, train_loader, val_loader, num_epochs=1, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义模型和对应的模型路径\n",
    "models_info = {\n",
    "    'CNN_Model': ('models/best_model_CNN_Model.pth', CNN_Model()),\n",
    "    'ResNet50_Model': ('models/best_model_ResNet50_Model.pth', ViT()),\n",
    "    'ViT': ('models/best_model_ViT.pth', ViT()),  # ViT 模型\n",
    "    'ResNet18': ('models/best_model_ResNet18.pth', ResNet18()),  # ResNet18 模型\n",
    "    'ResNeXt101': ('models/best_model_ResNeXt101.pth', ResNeXt101()),  # ResNeXt101 模型\n",
    "    'GoogLeNet': ('models/best_model_GoogLeNet.pth', GoogLeNet()),  # GoogLeNet 模型\n",
    "    # 你可以继续添加其他模型\n",
    "}\n",
    "\n",
    "# 创建一个函数，使用 Predictor 进行预测\n",
    "def run_prediction_for_models(models_info, test_loader, output_folder='predictions'):\n",
    "    # 确保输出文件夹存在\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for model_name, (model_path, model_instance) in models_info.items():\n",
    "        print(f\"正在预测模型: {model_name}\")\n",
    "        \n",
    "        # 初始化 Predictor 类，传入模型和模型权重路径\n",
    "        predictor = Predictor(model=model_instance, model_path=model_path)\n",
    "        \n",
    "        # 使用 predictor 对测试集进行预测并生成提交文件\n",
    "        output_file = f\"{output_folder}/submission_{model_name}.csv\"\n",
    "        predictor.predict(test_loader=test_loader, output_file=output_file)\n",
    "        \n",
    "        print(f\"模型 {model_name} 的预测完成并保存至: {output_file}\")\n",
    "\n",
    "# 假设你已经有 test_loader\n",
    "run_prediction_for_models(models_info, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
